{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/home/chenpz/.conda/envs/sft/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "Loading checkpoint shards: 100%|██████████| 4/4 [00:02<00:00,  1.40it/s]\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig,default_data_collator\n",
    "import torch\n",
    "from peft import PeftModel\n",
    "from transformers import GenerationConfig\n",
    "from transformers.generation.utils import LogitsProcessorList\n",
    "from transformers.generation.logits_process import LogitsProcessor\n",
    "from datasets import load_dataset\n",
    "from torch.utils.data import DataLoader\n",
    "import os\n",
    "\n",
    "abs_path = \"/data/home/chenpz/git_clone_project\"\n",
    "model_path = f\"{abs_path}/All_base_model/models--meta-llama--Meta-Llama-3-8B-Instruct/snapshots/d3aa29f914761e8ea0298051fbaf8dd173e94db5\"\n",
    "data_path = f\"/data/home/chenpz/git_clone_project/nlpData/mix_data_of_alpaca/FFP_all.json\"\n",
    "# adpter_path = f\"/data/home/chenpz/git_clone_project/LLaMA-Factory/saves/llama3-8b-anli_r3_train_kcg_addHighPPL_first5000_data_gas=5_lr=1e-4/checkpoint-60\"\n",
    "output_file = '/data/home/chenpz/git_clone_project/jupyter_notebook_test/output/mixed_alpaca_answerPPL_FFP_all.jsonl'\n",
    "\n",
    "output_dir = os.path.dirname(output_file)\n",
    "if not os.path.exists(output_dir):\n",
    "    os.makedirs(output_dir)\n",
    "\n",
    "\n",
    "device = 'cuda:1'\n",
    "prompt = \\\n",
    "'''<|begin_of_text|><|start_header_id|>user<|end_header_id|>\n",
    "\n",
    "{instruction} {input}<|eot_id|><|start_header_id|>assistant<|end_header_id|>\\n\\n{output}<|eot_id|>'''\n",
    "\n",
    "\n",
    "\n",
    "# nf4_config = BitsAndBytesConfig(\n",
    "#    load_in_4bit=True,\n",
    "#    bnb_4bit_quant_type=\"nf4\",\n",
    "#    bnb_4bit_use_double_quant=True,\n",
    "#    bnb_4bit_compute_dtype= None\n",
    "# )\n",
    "\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(model_path, \n",
    "                                             torch_dtype=torch.bfloat16,\n",
    "                                            #  quantization_config=nf4_config,\n",
    "                                             device_map = device)\n",
    "\n",
    "# model = PeftModel.from_pretrained(\n",
    "#                     model, adpter_path, is_trainable=False\n",
    "#                     )\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|>Here are the counties in Ireland:\n",
      "\n",
      "1. Carlow\n",
      "2. Cavan\n",
      "3. Clare\n",
      "4. Cork\n",
      "5. Donegal\n",
      "6. Dublin\n",
      "7. Galway\n",
      "8. Kerry\n",
      "9. Kildare\n",
      "10. Kilkenny\n",
      "11. Laois\n",
      "12. Leitrim\n",
      "13. Limerick\n",
      "14. Longford\n",
      "15. Louth\n",
      "16. Mayo\n",
      "17. Meath\n",
      "18. Monaghan\n",
      "19. Offaly\n",
      "20. Roscommon\n",
      "21. Sligo\n",
      "22. Tipperary\n",
      "23. Waterford\n",
      "24. Westmeath\n",
      "25. Wexford\n",
      "26. Wicklow<|eot_id|>\n"
     ]
    }
   ],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(model_path,use_fast=False,split_special_tokens=False,\n",
    "                                          padding_side=\"left\",\n",
    "                                          **{'trust_remote_code': True, 'cache_dir': None, 'revision': 'main', 'use_auth_token': None})\n",
    "tokenizer.pad_token = '<|eot_id|>'\n",
    "\n",
    "dataset = load_dataset(\"json\", data_files= data_path)\n",
    "\n",
    "IGNORE_INDEX = -100\n",
    "\n",
    "question_prompt = '''<|begin_of_text|><|start_header_id|>user<|end_header_id|>\n",
    "\n",
    "{instruction}{input}<|eot_id|><|start_header_id|>assistant<|end_header_id|>\\n\\n'''\n",
    "\n",
    "answer_prompt = '''{output}<|eot_id|>'''\n",
    "\n",
    "\n",
    "def preprocess_Answer_data(examples) :\n",
    "        model_inputs = {\"input_ids\": [], \"attention_mask\": [], \"labels\": []}\n",
    "        max_length = 400\n",
    "        for  output in examples['output']:\n",
    "\n",
    "            response= answer_prompt.format(output = output)\n",
    "            \n",
    "            input_ids, labels = [], []\n",
    "            target_ids = tokenizer(response)['input_ids']\n",
    "            input_ids +=  target_ids\n",
    "\n",
    "            input_ids = input_ids[:max_length]\n",
    "\n",
    "            labels += target_ids\n",
    "            labels = labels[:max_length]\n",
    "            \n",
    "            model_inputs[\"input_ids\"].append(input_ids)\n",
    "            model_inputs[\"attention_mask\"].append([1] * len(input_ids))\n",
    "            model_inputs[\"labels\"].append(labels)\n",
    "\n",
    "        # Padding: Ensure that all input sequences have the same length\n",
    "        for i in range(len(model_inputs[\"input_ids\"])):\n",
    "            padding_length = max_length - len(model_inputs[\"input_ids\"][i])\n",
    "            \n",
    "            # Left padding input_ids and attention_mask\n",
    "            model_inputs[\"input_ids\"][i] = [tokenizer.pad_token_id] * padding_length + model_inputs[\"input_ids\"][i]\n",
    "            model_inputs[\"attention_mask\"][i] = [0] * padding_length + model_inputs[\"attention_mask\"][i]\n",
    "\n",
    "            # Left padding labels with IGNORE_INDEX\n",
    "            model_inputs[\"labels\"][i] = [IGNORE_INDEX] * padding_length + model_inputs[\"labels\"][i]\n",
    "        \n",
    "        return model_inputs\n",
    "\n",
    "\n",
    "dataset2 = dataset.map(preprocess_Answer_data,batched=True,remove_columns=['output', 'input', 'instruction'],num_proc=16)\n",
    "print(tokenizer.decode(dataset2['train'][0]['input_ids']))\n",
    "\n",
    "eval_dataloader = DataLoader(dataset2['train'],batch_size=6, pin_memory=True,collate_fn=default_data_collator,shuffle=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 20294/20294 [1:37:39<00:00,  3.46it/s]\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "model.eval()\n",
    "loss_list =[]\n",
    "loss_fn = torch.nn.CrossEntropyLoss(\n",
    "    reduction='none'\n",
    "    ) # 不要平均，保留每个 token 的 loss\n",
    "with open(output_file, 'w') as file:  # Open the file in write mode before the loop\n",
    "    for step, batch in enumerate(tqdm(eval_dataloader)):\n",
    "        batch = {k: v.to(device) for k, v in batch.items()}\n",
    "        with torch.no_grad():\n",
    "            outputs = model(input_ids=batch['input_ids'], attention_mask=batch['attention_mask'], labels=batch['labels'])\n",
    "\n",
    "            logits = outputs.logits\n",
    "            shift_logits = logits[..., :-1, :].contiguous()  # 移动 logits 以对齐预测\n",
    "            shift_labels = batch['labels'][:, 1:].contiguous()  # 移动 labels\n",
    "            \n",
    "            # 计算每个 token 的 loss\n",
    "            loss_per_token = loss_fn(shift_logits.view(-1, model.config.vocab_size), shift_labels.view(-1))\n",
    "            # reshape 回到 batch 大小\n",
    "            loss_per_token = loss_per_token.view(shift_labels.size())\n",
    "            \n",
    "\n",
    "            means = loss_per_token.sum(dim=-1) / (loss_per_token != 0).sum(dim=-1)\n",
    "            means = list( means.cpu().numpy() )\n",
    "\n",
    "        # break\n",
    "            # print(loss_per_sample)\n",
    "            for i, loss in enumerate(means):\n",
    "                # print(loss)\n",
    "                # break\n",
    "                sample_id = step * eval_dataloader.batch_size + i  # 用于区分每个样本的索引\n",
    "                add_item = {'id':sample_id,'loss':loss}\n",
    "                file.write('%s\\n' % add_item)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "means.cpu().numpy()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "sft",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
