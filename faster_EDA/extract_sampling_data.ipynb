{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/home/chenpz/.conda/envs/sft/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "dataset = load_dataset(\"/data/home/chenpz/git_clone_project/nlpData/p3/anli_can_we_infer_r3\")\n",
    "train_data = dataset['train']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "with open('/data/home/chenpz/git_clone_project/jupyter_notebook_test/output/10times_500kcenters.pkl', 'rb') as f:\n",
    "    # 使用 pickle.load() 函数加载数据结构\n",
    "    centers = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = dataset['train']\n",
    "res = []\n",
    "for i,j in zip(x['targets_pretokenized'],x['inputs_pretokenized']):\n",
    "    add_item = {'instruction':j,\"input\":\"\",'output':i}\n",
    "    res.append(add_item)\n",
    "with open(f'/data/home/chenpz/git_clone_project/nlpData/p3/anli_r3_json_file/anli_r3_train.json','w') as f:\n",
    "    json.dump(res,f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = dataset['validation']\n",
    "res = []\n",
    "for i,j in zip(x['targets_pretokenized'],x['inputs_pretokenized']):\n",
    "    add_item = {'instruction':j,\"input\":\"\",'output':i}\n",
    "    res.append(add_item)\n",
    "with open(f'/data/home/chenpz/git_clone_project/nlpData/p3/anli_r3_json_file/anli_r3_vaild.json','w') as f:\n",
    "    json.dump(res,f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = dataset['test']\n",
    "res = []\n",
    "for i,j in zip(x['targets_pretokenized'],x['inputs_pretokenized']):\n",
    "    add_item = {'instruction':j,\"input\":\"\",'output':i}\n",
    "    res.append(add_item)\n",
    "with open(f'/data/home/chenpz/git_clone_project/nlpData/p3/anli_r3_json_file/anli_r3_test.json','w') as f:\n",
    "    json.dump(res,f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "with open('/data/home/chenpz/git_clone_project/jupyter_notebook_test/output/1times_50000kcenters_seed=1024.pkl', 'rb') as f:\n",
    "    # 使用 pickle.load() 函数加载数据结构\n",
    "    center_50000 = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "center_5000 = center_50000[50:5050]\n",
    "x = dataset['train'][center_5000]\n",
    "res = []\n",
    "for i,j in zip(x['targets_pretokenized'],x['inputs_pretokenized']):\n",
    "    add_item = {'instruction':j,\"input\":\"\",'output':i}\n",
    "    res.append(add_item)\n",
    "    \n",
    "# from random import shuffle\n",
    "# shuffle(res)\n",
    "with open(f'/data/home/chenpz/git_clone_project/nlpData/p3/anli_r3_json_file/anli_r3_train_5000sampling_seed=1024_shift50.json','w') as f:\n",
    "    json.dump(res,f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'instruction': 'Given Howl-O-Scream is an annual special event that occurs at Busch Gardens Tampa Bay, Busch Gardens Williamsburg, and SeaWorld San Antonio. The parks remain operational during the day and transition to Howl-O-Scream at night. The event features haunted houses, \"scare zones\", and live entertainment. It is a seasonal event that occurs in the Fall of the year, tied to Halloween. Is it guaranteed true that \"You can find Scare Zones at Howl-O-Scream events\"? Yes, no, or maybe? ',\n",
       " 'input': '',\n",
       " 'output': ' Yes '}"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "x = dataset['train'][center_50000]\n",
    "res = []\n",
    "for i,j in zip(x['targets_pretokenized'],x['inputs_pretokenized']):\n",
    "    add_item = {'instruction':j,\"input\":\"\",'output':i}\n",
    "    res.append(add_item)\n",
    "    \n",
    "from random import shuffle\n",
    "shuffle(res)\n",
    "with open(f'/data/home/chenpz/git_clone_project/nlpData/p3/anli_r3_json_file/anli_r3_train_50000sampling_seed=1024_shuffle.json','w') as f:\n",
    "    json.dump(res,f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "x = dataset['train'][center_50000]\n",
    "res = []\n",
    "for i,j in zip(x['targets_pretokenized'],x['inputs_pretokenized']):\n",
    "    add_item = {'instruction':j,\"input\":\"\",'output':i}\n",
    "    res.append(add_item)\n",
    "\n",
    "with open(f'/data/home/chenpz/git_clone_project/nlpData/p3/anli_r3_json_file/anli_r3_train_50000sampling_seed=1024.json','w') as f:\n",
    "    json.dump(res,f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 提取非前K center"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "bad_data = list(set(range(len(train_data))) - set(center_50000))\n",
    "x = dataset['train'][bad_data]\n",
    "res = []\n",
    "for i,j in zip(x['targets_pretokenized'],x['inputs_pretokenized']):\n",
    "    add_item = {'instruction':j,\"input\":\"\",'output':i}\n",
    "    res.append(add_item)\n",
    "\n",
    "with open(f'/data/home/chenpz/git_clone_project/nlpData/p3/anli_r3_json_file/anli_r3_train_50000_bad_data.json','w') as f:\n",
    "    json.dump(res,f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 提取far初始化的前K"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "with open('/data/home/chenpz/git_clone_project/jupyter_notebook_test/output/kcenters_rank_fromFarPoint.pkl', 'rb') as f:\n",
    "    # 使用 pickle.load() 函数加载数据结构\n",
    "    res = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import json\n",
    "with open('/data/home/chenpz/git_clone_project/jupyter_notebook_test/output/kcenters_rank_fromFarPoint.pkl', 'rb') as f:\n",
    "    # 使用 pickle.load() 函数加载数据结构\n",
    "    res = pickle.load(f)\n",
    "\n",
    "center_5000 = res[:5000]\n",
    "# import random\n",
    "# random.shuffle(center_5000)\n",
    "x = dataset['train'][center_5000]\n",
    "\n",
    "res1 = []\n",
    "for i,j in zip(x['targets_pretokenized'],x['inputs_pretokenized']):\n",
    "    add_item = {'instruction':j,\"input\":\"\",'output':i}\n",
    "    res1.append(add_item)\n",
    "with open(f'/data/home/chenpz/git_clone_project/nlpData/p3/anli_r3_json_file/anli_r3_train_kcg_first_5000.json','w') as f:\n",
    "    json.dump(res1,f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import json\n",
    "with open('/data/home/chenpz/git_clone_project/jupyter_notebook_test/output/kcenters_rank_fromFarPoint.pkl', 'rb') as f:\n",
    "    # 使用 pickle.load() 函数加载数据结构\n",
    "    res = pickle.load(f)\n",
    "\n",
    "center_5000 = res[:5000]\n",
    "import random\n",
    "random.shuffle(center_5000)\n",
    "x = dataset['train'][center_5000]\n",
    "\n",
    "res1 = []\n",
    "for i,j in zip(x['targets_pretokenized'],x['inputs_pretokenized']):\n",
    "    add_item = {'instruction':j,\"input\":\"\",'output':i}\n",
    "    res1.append(add_item)\n",
    "with open(f'/data/home/chenpz/git_clone_project/nlpData/p3/anli_r3_json_file/anli_r3_train_5000_fromFarPoint_shuffle.json','w') as f:\n",
    "    json.dump(res1,f)\n",
    "\n",
    "\n",
    "# center_50000 = res[:50000]\n",
    "# x = dataset['train'][center_50000]\n",
    "# res2 = []\n",
    "# for i,j in zip(x['targets_pretokenized'],x['inputs_pretokenized']):\n",
    "#     add_item = {'instruction':j,\"input\":\"\",'output':i}\n",
    "#     res2.append(add_item)\n",
    "\n",
    "# with open(f'/data/home/chenpz/git_clone_project/nlpData/p3/anli_r3_json_file/anli_r3_train_50000_fromFarPoint.json','w') as f:\n",
    "#     json.dump(res2,f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 提取ppl最高和最低的数据"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import json\n",
    "\n",
    "x = []\n",
    "y = []\n",
    "# 从 JSON 文件读取数据\n",
    "with open('/data/home/chenpz/git_clone_project/jupyter_notebook_test/output/anli_r3_loss_FFP_all.jsonl', 'r', encoding='utf-8') as json_file:\n",
    "    for line in json_file:\n",
    "        line = line.strip().replace(\"'\", '\"')\n",
    "        data = json.loads(line)\n",
    "        x.append(data['id'])\n",
    "        y.append(data['loss'])\n",
    "\n",
    "\n",
    "import pickle\n",
    "import json\n",
    "with open('/data/home/chenpz/git_clone_project/jupyter_notebook_test/output/kcenters_rank_fromFarPoint.pkl', 'rb') as f:\n",
    "    # 使用 pickle.load() 函数加载数据结构\n",
    "    res = pickle.load(f)\n",
    "\n",
    "index_dict = {idx: loss for loss, idx in zip(y,res)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(33423, 15.21229362487793)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sorted(index_dict.items(), key=lambda item: item[1])[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(7.484609794998169, 11.634144786071777, 9.21622926235199)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.mean(sorted(y)[:5000]), np.mean(sorted(y)[-5000:]), np.mean(y[50000:55000])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "sorted_dict = dict(sorted(index_dict.items(), key=lambda item: item[1]))\n",
    "data = list(sorted_dict.keys())\n",
    "low_ppl_data = data[:5000]\n",
    "high_ppl_data = data[-5000:]\n",
    "\n",
    "res = low_ppl_data + high_ppl_data\n",
    "\n",
    "x = dataset['train'][res]\n",
    "res1 = []\n",
    "for i,j in zip(x['targets_pretokenized'],x['inputs_pretokenized']):\n",
    "    add_item = {'instruction':j,\"input\":\"\",'output':i}\n",
    "    res1.append(add_item)\n",
    "with open(f'/data/home/chenpz/git_clone_project/nlpData/p3/anli_r3_json_file/anli_r3_train_high_low_ppl_10000.json','w') as f:\n",
    "    json.dump(res1,f)\n",
    "\n",
    "# x = dataset['train'][high_ppl_data]\n",
    "# res1 = []\n",
    "# for i,j in zip(x['targets_pretokenized'],x['inputs_pretokenized']):\n",
    "#     add_item = {'instruction':j,\"input\":\"\",'output':i}\n",
    "#     res1.append(add_item)\n",
    "# with open(f'/data/home/chenpz/git_clone_project/nlpData/p3/anli_r3_json_file/anli_r3_train_5000_high_ppl_data.json','w') as f:\n",
    "#     json.dump(res1,f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "random.shuffle(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# IFD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import json\n",
    "\n",
    "x = []\n",
    "y = []\n",
    "# 从 JSON 文件读取数据\n",
    "# /data/home/chenpz/git_clone_project/jupyter_notebook_test/output/anli_r3_loss_FFP_all.jsonl\n",
    "# /data/home/chenpz/git_clone_project/jupyter_notebook_test/output/anli_r3_loss_trained_By_kcg_addHighPPL_first5000_data_FFP_all.jsonl\n",
    "with open('/data/home/chenpz/git_clone_project/jupyter_notebook_test_data_selection/output/anli_r3_PPL_FFP_all.jsonl', 'r', encoding='utf-8') as json_file:\n",
    "    for line in json_file:\n",
    "        line = line.strip().replace(\"'\", '\"')\n",
    "        data = json.loads(line)\n",
    "        x.append(data['id'])\n",
    "        y.append(data['loss'])\n",
    "\n",
    "z = []\n",
    "with open('/data/home/chenpz/git_clone_project/jupyter_notebook_test_data_selection/output/anli_r3_answerPPL_FFP_all.jsonl', 'r', encoding='utf-8') as json_file:\n",
    "    for line in json_file:\n",
    "        line = line.strip().replace(\"'\", '\"')\n",
    "        data = json.loads(line)\n",
    "        z.append(data['loss'])\n",
    "\n",
    "# IFD\n",
    "y = np.array(y)/np.array(z)\n",
    "\n",
    "\n",
    "import pickle\n",
    "import json\n",
    "with open('/data/home/chenpz/git_clone_project/jupyter_notebook_test_data_selection/output/kcenters_rank_fromFarPoint.pkl', 'rb') as f:\n",
    "    # 使用 pickle.load() 函数加载数据结构\n",
    "    res = pickle.load(f)\n",
    "\n",
    "index_dict = {idx: loss for loss, idx in zip(y,res)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "sorted_dict = dict(sorted(index_dict.items(), key=lambda item: item[1]))\n",
    "data = list(sorted_dict.keys())\n",
    "low_IFD_data = data[:5000]\n",
    "high_IFD_data = data[-10000:-5000]\n",
    "\n",
    "res = low_IFD_data + high_IFD_data\n",
    "\n",
    "x = dataset['train'][high_IFD_data]\n",
    "res1 = []\n",
    "for i,j in zip(x['targets_pretokenized'],x['inputs_pretokenized']):\n",
    "    add_item = {'instruction':j,\"input\":\"\",'output':i}\n",
    "    res1.append(add_item)\n",
    "with open(f'/data/home/chenpz/git_clone_project/nlpData/p3/anli_r3_json_file/anli_r3_train_high_IFD_5000~10000.json','w') as f:\n",
    "    json.dump(res1,f)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ppl high - kcg last"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "sorted_dict = dict(sorted(index_dict.items(), key=lambda item: item[1]))\n",
    "data = list(sorted_dict.keys())\n",
    "kcg_first = res[:5000]\n",
    "high_ppl_data = data[-5000:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(1000):\n",
    "    kcg_first = res[0:5000]\n",
    "    bad_datas = set(kcg_first).intersection(set(high_ppl_data))\n",
    "    remain_data = list(set(kcg_first) - bad_datas)\n",
    "    print(len(bad_datas))\n",
    "    if len(remain_data) == 5000:\n",
    "        print(i)\n",
    "        print('yes')\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "305\n"
     ]
    }
   ],
   "source": [
    "bad_datas = set(kcg_first).intersection(set(high_ppl_data))\n",
    "print(len(bad_datas))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "327"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "high_ppl_data = data[-5317:]\n",
    "kcg_last = res[-5317:]\n",
    "len(set(kcg_last).intersection(set(high_ppl_data)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# len(set(kcg_last).intersection(set(high_ppl_data)))\n",
    "high_ppl_data = data[-5000:]\n",
    "kcg_last = res[-5000-327:]\n",
    "bad_datas = list( set(kcg_last).intersection(set(high_ppl_data)))\n",
    "kcg_first = res[: (5000-len(bad_datas)) ] + bad_datas\n",
    "x = dataset['train'][kcg_first]\n",
    "res1 = []\n",
    "for i,j in zip(x['targets_pretokenized'],x['inputs_pretokenized']):\n",
    "    add_item = {'instruction':j,\"input\":\"\",'output':i}\n",
    "    res1.append(add_item)\n",
    "with open(f'/data/home/chenpz/git_clone_project/nlpData/p3/anli_r3_json_file/anli_r3_train_kcg_addHighPPL_first5000_data.json','w') as f:\n",
    "    json.dump(res1,f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 提取FPS的后5000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import json\n",
    "from datasets import load_dataset\n",
    "\n",
    "dataset = load_dataset(\"/data/home/chenpz/git_clone_project/nlpData/p3/anli_can_we_infer_r3\")\n",
    "\n",
    "with open('/data/home/chenpz/git_clone_project/jupyter_notebook_test/output/anli_r3_fps_rank.pkl', 'rb') as f:\n",
    "    # 使用 pickle.load() 函数加载数据结构\n",
    "    res = pickle.load(f)\n",
    "\n",
    "center_5000 = res[0][:5000]\n",
    "\n",
    "# print(len(center_5000))\n",
    "x = dataset['train'][center_5000]\n",
    "\n",
    "res1 = []\n",
    "for i,j in zip(x['targets_pretokenized'],x['inputs_pretokenized']):\n",
    "    add_item = {'instruction':j,\"input\":\"\",'output':i}\n",
    "    res1.append(add_item)\n",
    "with open(f'/data/home/chenpz/git_clone_project/nlpData/p3/anli_r3_json_file/anli_r3_train_fps_first_5000.json','w') as f:\n",
    "    json.dump(res1,f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 随机取点"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from random import sample\n",
    "idx = list(range(len(dataset['train'])))\n",
    "new_idx = sample(idx, 5000)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = dataset['train'][new_idx]\n",
    "res = []\n",
    "for i,j in zip(x['targets_pretokenized'],x['inputs_pretokenized']):\n",
    "    add_item = {'instruction':j,\"input\":\"\",'output':i}\n",
    "    res.append(add_item)\n",
    "with open(f'/data/home/chenpz/git_clone_project/nlpData/p3/anli_r3_json_file/anli_r3_train_random_5000samping.json','w') as f:\n",
    "    json.dump(res,f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# FFP_前4600"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import json\n",
    "with open('/data/home/chenpz/git_clone_project/jupyter_notebook_test/output/kcenters_rank_fromFarPoint.pkl', 'rb') as f:\n",
    "    # 使用 pickle.load() 函数加载数据结构\n",
    "    res = pickle.load(f)\n",
    "\n",
    "center_5000 = res[:4600]\n",
    "\n",
    "x = dataset['train'][center_5000]\n",
    "\n",
    "res1 = []\n",
    "for i,j in zip(x['targets_pretokenized'],x['inputs_pretokenized']):\n",
    "    add_item = {'instruction':j,\"input\":\"\",'output':i}\n",
    "    res1.append(add_item)\n",
    "with open(f'/data/home/chenpz/git_clone_project/nlpData/p3/anli_r3_json_file/anli_r3_train_4600_FFP.json','w') as f:\n",
    "    json.dump(res1,f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# FFP_前5000 + 后5000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import json\n",
    "with open('/data/home/chenpz/git_clone_project/jupyter_notebook_test/output/kcenters_rank_fromFarPoint.pkl', 'rb') as f:\n",
    "    # 使用 pickle.load() 函数加载数据结构\n",
    "    res = pickle.load(f)\n",
    "\n",
    "center_5000 = res[:5000]\n",
    "center_5000 += res[-5000:]\n",
    "x = dataset['train'][center_5000]\n",
    "\n",
    "res1 = []\n",
    "for i,j in zip(x['targets_pretokenized'],x['inputs_pretokenized']):\n",
    "    add_item = {'instruction':j,\"input\":\"\",'output':i}\n",
    "    res1.append(add_item)\n",
    "with open(f'/data/home/chenpz/git_clone_project/nlpData/p3/anli_r3_json_file/anli_r3_train_kcg_first+last5000.json','w') as f:\n",
    "    json.dump(res1,f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import json\n",
    "from datasets import load_dataset\n",
    "with open('/data/home/chenpz/git_clone_project/jupyter_notebook_test/output/anli_r3_kcg_seed1024.pkl', 'rb') as f:\n",
    "    # 使用 pickle.load() 函数加载数据结构\n",
    "    res = pickle.load(f)\n",
    "\n",
    "import json\n",
    "\n",
    "# json_data = []\n",
    "dataset = load_dataset(\"/data/home/chenpz/git_clone_project/nlpData/p3/anli_can_we_infer_r3\")\n",
    "\n",
    "\n",
    "\n",
    "center_5000 = res[0][-5000:]\n",
    "x = dataset['train'][center_5000]\n",
    "\n",
    "res1 = []\n",
    "for i,j in zip(x['targets_pretokenized'],x['inputs_pretokenized']):\n",
    "    add_item = {'instruction':j,\"input\":\"\",'output':i}\n",
    "    res1.append(add_item)\n",
    "\n",
    "len(res1)\n",
    "with open(f'/data/home/chenpz/git_clone_project/nlpData/p3/anli_r3_json_file/anli_r3_train_kcg_last_5000_seed1024.json','w') as f:\n",
    "    json.dump(res1,f)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "sft",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
