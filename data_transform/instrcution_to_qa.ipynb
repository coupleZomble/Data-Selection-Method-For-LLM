{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# from json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "\n",
    "instruction_data_path = '/data/home/chenpz/git_clone_project/nlpData/FLAN/squad_v2/squad_v2_10templates_train.jsonl'\n",
    "instruction_dataset = load_dataset(\"json\", data_files=instruction_data_path)\n",
    "train_data = instruction_dataset['train']\n",
    "qa_data_path = '/data/home/chenpz/git_clone_project/jupyter_notebook_test/output/dolly_v2_qa.json' \n",
    "\n",
    "res = []\n",
    "for instruction, output in zip(train_data['instruction'],train_data['output']):\n",
    "    str1 = \\\n",
    "    f'''\n",
    "Instruction:\\n{instruction}\\n\n",
    "Response:\\n{output}\\n\n",
    "    '''\n",
    "    add_item = {'question':str1}\n",
    "    res.append(add_item)\n",
    "\n",
    "import json\n",
    "with open(qa_data_path,'w') as f:\n",
    "    json.dump(res,f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "({'question': '\\nInstruction:\\n Explain the use of screens in basketball\\n\\nResponse:\\nIn basketball, when a team has the ball, one person may set a screen by blocking a defender with his body but making sure his hands are not extended. This allows the person with the ball to pass, shoot, etc. without issuing any fouls. If the person setting a screen moves while screening, the referee will call a moving screen and the ball will be inbounded by the person who moved while screening from the half court line. In any case, screens do not foul the screener for contact unless they are to do physical contact with their hands or feet.\\n\\n    '},\n",
       " {'question': '\\nInstruction:\\n Explain the use of screens in basketball\\n\\nResponse:\\nIn basketball, when a team has the ball, one person may set a screen by blocking a defender with his body but making sure his hands are not extended. This allows the person with the ball to pass, shoot, etc. without issuing any fouls. If the person setting a screen moves while screening, the referee will call a moving screen and the ball will be inbounded by the person who moved while screening from the half court line. In any case, screens do not foul the screener for contact unless they are to do physical contact with their hands or feet.\\n\\n    '},\n",
       " {'question': '\\nInstruction:\\n Which is a species of fish? Tope or Rope\\n\\nResponse:\\nTope\\n\\n    '})"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res[4710],res[6342],res[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# from jsonl to json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# /data/home/chenpz/git_clone_project/nlpData/dolly_v2/alpaca_evol_instruct_70k.json\n",
    "import json\n",
    "\n",
    "def jsonl_to_json(jsonl_file):\n",
    "    json_data = []\n",
    "    with open(jsonl_file, 'r', encoding='utf-8') as f:\n",
    "        for line in f:\n",
    "            try:\n",
    "                obj = json.loads(line)\n",
    "                add_item = {\"instruction\":obj['inputs'], \"input\":\"\", \"output\":obj['targets']}\n",
    "                json_data.append(add_item)\n",
    "            except json.JSONDecodeError:\n",
    "                print(f\"Ignoring invalid JSON object: {line}\")\n",
    "    return json_data\n",
    "    # with open(json_file, 'w', encoding='utf-8') as f:\n",
    "    #     json.dump(json_data, f, indent=4)\n",
    "\n",
    "jsonl_file = '/data/home/chenpz/git_clone_project/nlpData/FLAN/squad_v2/squad_v2_10templates_train.jsonl'\n",
    "# json_file = '/data/home/chenpz/git_clone_project/nlpData/dolly_v2/databricks-dolly-15k.json'\n",
    "\n",
    "json_data = jsonl_to_json(jsonl_file)\n",
    "\n",
    "\n",
    "json_file = '/data/home/chenpz/git_clone_project/nlpData/FLAN/squad_v2/squad_v2_instruction.json'\n",
    "with open(json_file, 'w', encoding='utf-8') as f:\n",
    "    json.dump(json_data, f, indent=4)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'instruction': 'The archipelago is formed by high points on the rim of the caldera of a submarine volcano that forms a seamount. The volcano is one part of a range that was formed as part of the same process that formed the floor of the Atlantic, and the Mid-Atlantic Ridge. The top of the seamount has gone through periods of complete submergence, during which its limestone cap was formed by marine organisms, and during the Ice Ages the entire caldera was above sea level, forming an island of approximately two hundred square miles.\\n\\nAnswer this question, if possible (if impossible, reply \"unanswerable\"): The submarine volcano was formed as part of the same proces as what two things?',\n",
       " 'inputs': '',\n",
       " 'output': 'the floor of the Atlantic, and the Mid-Atlantic Ridge'}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "json_data[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# parquet to json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "dataset = load_dataset(\"parquet\",data_files=\"/data/home/chenpz/git_clone_project/nlpData/ag_news/data/train-00000-of-00001.parquet\")\n",
    "dataset['train'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating validation split: 70 examples [00:00, 2348.28 examples/s]\n",
      "Generating test split: 12032 examples [00:00, 440123.37 examples/s]\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "dataset = load_dataset(\"/data/home/chenpz/git_clone_project/nlpData/mmlu_pro\")\n",
    "# train_data = dataset['train']\n",
    "\n",
    "# res = []\n",
    "# for i,j in zip(dataset['train']['targets_pretokenized'],dataset['train']['inputs_pretokenized']):\n",
    "#     add_item = {'question':j,'answer':i}\n",
    "#     res.append(add_item)\n",
    "# import json\n",
    "# with open('/data/home/chenpz/git_clone_project/jupyter_notebook_test/output/anli_can_we_infer_r3_train.json','w') as f:\n",
    "#     json.dump(res,f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# change keys for dolly_v2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/home/chenpz/.conda/envs/llama_factory/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "\n",
    "instruction_data_path = '/data/home/chenpz/git_clone_project/nlpData/dolly_v2/databricks-dolly-15k.json'\n",
    "instruction_dataset = load_dataset(\"json\", data_files=instruction_data_path)\n",
    "train_data = instruction_dataset['train']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_data(examples) :\n",
    "    \n",
    "    # for  instruction, context, response in zip(examples['instruction'],examples['context'],examples['response']):\n",
    "    # instruction = ''.join(examples['instruction'])\n",
    "    # context = ''.join(examples['context'])\n",
    "    input_context = f'''{examples['context']} {examples['instruction']}'''\n",
    "    model_inputs = {\"instruction\": [],\"input\":[], \"output\":[]}\n",
    "    model_inputs['instruction']=input_context\n",
    "    model_inputs['output']=examples['response']\n",
    "    return model_inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map (num_proc=16): 100%|██████████| 15011/15011 [00:00<00:00, 72280.30 examples/s]\n"
     ]
    }
   ],
   "source": [
    "dataset2 = train_data.map(preprocess_data,remove_columns=['category','context','response'],num_proc=16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'instruction': \"Virgin Australia, the trading name of Virgin Australia Airlines Pty Ltd, is an Australian-based airline. It is the largest airline by fleet size to use the Virgin brand. It commenced services on 31 August 2000 as Virgin Blue, with two aircraft on a single route. It suddenly found itself as a major airline in Australia's domestic market after the collapse of Ansett Australia in September 2001. The airline has since grown to directly serve 32 cities in Australia, from hubs in Brisbane, Melbourne and Sydney. When did Virgin Australia start operating?\",\n",
       " 'input': [],\n",
       " 'output': 'Virgin Australia commenced services on 31 August 2000 as Virgin Blue, with two aircraft on a single route.'}"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset2[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Creating json from Arrow format: 100%|██████████| 16/16 [00:00<00:00, 154.77ba/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "12550726"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset2.to_json(\"/data/home/chenpz/git_clone_project/nlpData/dolly_v2/Instrcution_form.json\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 镜像"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using the latest cached version of the dataset since fancyzhx/ag_news couldn't be found on the Hugging Face Hub\n",
      "Found the latest cached dataset configuration 'default' at /data/home/chenpz/git_clone_project/nlpData/fancyzhx___ag_news/default/0.0.0/eb185aade064a813bc0b7f42de02595523103ca4 (last modified on Fri Nov 15 06:25:21 2024).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'text': \"Wall St. Bears Claw Back Into the Black (Reuters) Reuters - Short-sellers, Wall Street's dwindling\\\\band of ultra-cynics, are seeing green again.\", 'label': 2}\n"
     ]
    }
   ],
   "source": [
    "# import os\n",
    "\n",
    "\n",
    "# export HF_ENDPOINT=https://hf-mirror.com\n",
    "# Load a dataset from the Hugging Face Hub\n",
    "from datasets import load_dataset\n",
    "dataset = load_dataset('fancyzhx/ag_news', cache_dir = '/data/home/chenpz/git_clone_project/nlpData')\n",
    "\n",
    "# Print the first example in the dataset\n",
    "print(dataset['train'][0])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llama_factory",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
